# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Osu4EMpSNaEhHM121x-hPTy8whHr6V3R
"""

import subprocess
import sys
def install_package(package):
    try:
        __import__(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install_package("numpy")
install_package("pandas")
install_package("joblib")
install_package("h5py")
install_package("json")
install_package("os")
install_package("matplotlib")
install_package("shap")
install_package("scikit-learn")
install_package("catboost")
install_package("xgboost")
install_package("scipy")


#Import necessary libraries
import numpy as np
import pandas as pd
import joblib
import h5py
import json
import os
import matplotlib.pyplot as plt
import shap
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_curve,
    auc,
    precision_recall_curve,
    average_precision_score
)
from catboost import CatBoostClassifier, Pool
from xgboost import XGBClassifier
from scipy.io import loadmat
import time

plt.style.use('seaborn-v0_8-whitegrid')
PLT_DPI = 300

# Configuration and File Paths
data_file_path = 'extracted_wavelet_features.mat'
metadata_file_path = 'metadata_features_labels.json'
catboost_model_save_path = 'catboost_fault_model.joblib'
xgboost_model_save_path = 'xgboost_fault_model.joblib'
scaler_save_path = 'feature_scaler.joblib'
results_save_path = 'comparative_ml_results.json'

print("--- Loading data and metadata ---")

try:
    with h5py.File(data_file_path, 'r') as f:
        features = np.array(f['all_features_matrix']).T
        labels_numeric = np.array(f['all_labels_numeric']).flatten()
except Exception as e:
    print(f"Error loading data with h5py: {e}. Falling back to scipy.")
    mat_data = loadmat(data_file_path)
    features = mat_data['all_features_matrix']
    labels_numeric = mat_data['all_labels_numeric'].flatten()
    if features.shape[0] < features.shape[1]:
        features = features.T

with open(metadata_file_path, 'r') as f:
    metadata = json.load(f)
    label_to_numeric = metadata['label_to_numeric']
    numeric_to_label = {v: k for k, v in label_to_numeric.items()}
    target_names = [numeric_to_label[i] for i in range(len(numeric_to_label))]
    feature_names_list = metadata['feature_names']

print(f"Number of features: {len(feature_names_list)}")
print(f"Data loaded. Features shape: {features.shape}, Labels shape: {labels_numeric.shape}")

# Prepare Data for Training
X_df = pd.DataFrame(features, columns=feature_names_list)
y = pd.Series(labels_numeric)

# Split data into a main training set for CV and a final holdout test set
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X_df, y, test_size=0.2, random_state=42, stratify=y
)

# K-Fold Cross-Validation
N_FOLDS = 5
skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)

cat_metrics_by_fold = []
xgb_metrics_by_fold = []

print(f"\n--- Starting {N_FOLDS}-Fold Stratified Cross-Validation ---")
for fold, (train_index, val_index) in enumerate(skf.split(X_train_full, y_train_full)):
    print(f"--- Processing Fold {fold + 1}/{N_FOLDS} ---")
    X_train, X_val = X_train_full.iloc[train_index], X_train_full.iloc[val_index]
    y_train, y_val = y_train_full.iloc[train_index], y_train_full.iloc[val_index]

    # Scale data for the current fold
    scaler_fold = StandardScaler()
    X_train_scaled = pd.DataFrame(scaler_fold.fit_transform(X_train), columns=X_train.columns)
    X_val_scaled = pd.DataFrame(scaler_fold.transform(X_val), columns=X_val.columns)

    # CatBoost Model Training (for this fold)
    cat_model_fold = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=8, loss_function='MultiClass',
                                        eval_metric='MultiClass', random_seed=42, early_stopping_rounds=50, verbose=0)
    cat_model_fold.fit(
        Pool(X_train_scaled, y_train, feature_names=list(X_train_scaled.columns)),
        eval_set=Pool(X_val_scaled, y_val),
        verbose=False
    )
    y_pred_cat_fold = cat_model_fold.predict(X_val_scaled).flatten()
    cat_metrics_by_fold.append(classification_report(y_val, y_pred_cat_fold, target_names=target_names, output_dict=True, zero_division=0))

    # XGBoost Model Training (for this fold)
    xgb_model_fold = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=8, objective='multi:softmax',
                                   num_class=len(target_names), eval_metric='merror', random_state=42, n_jobs=-1,
                                   early_stopping_rounds=50, use_label_encoder=False)
    xgb_model_fold.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=False)
    y_pred_xgb_fold = xgb_model_fold.predict(X_val_scaled)
    xgb_metrics_by_fold.append(classification_report(y_val, y_pred_xgb_fold, target_names=target_names, output_dict=True, zero_division=0))

print("\n--- Cross-Validation Complete. Aggregating Results ---")
# Function to aggregate metrics across all folds
def aggregate_metrics(metrics_by_fold, target_names):
    agg_metrics = {
        'accuracy': [],
        'macro_avg': {'precision': [], 'recall': [], 'f1-score': []},
        'weighted_avg': {'precision': [], 'recall': [], 'f1-score': []},
        'per_class': {name: {'precision': [], 'recall': [], 'f1-score': []} for name in target_names}
    }
    for report in metrics_by_fold:
        agg_metrics['accuracy'].append(report['accuracy'])
        for metric_name in ['precision', 'recall', 'f1-score']:
            agg_metrics['macro_avg'][metric_name].append(report['macro avg'][metric_name])
            agg_metrics['weighted_avg'][metric_name].append(report['weighted avg'][metric_name])
            for class_name in target_names:
                agg_metrics['per_class'][class_name][metric_name].append(report[class_name][metric_name])

    # Calculate mean and std for each metric
    final_metrics = {
        'accuracy_mean': np.mean(agg_metrics['accuracy']),
        'accuracy_std': np.std(agg_metrics['accuracy']),
        'macro_avg_mean': {m: np.mean(agg_metrics['macro_avg'][m]) for m in agg_metrics['macro_avg']},
        'macro_avg_std': {m: np.std(agg_metrics['macro_avg'][m]) for m in agg_metrics['macro_avg']},
        'weighted_avg_mean': {m: np.mean(agg_metrics['weighted_avg'][m]) for m in agg_metrics['weighted_avg']},
        'weighted_avg_std': {m: np.std(agg_metrics['weighted_avg'][m]) for m in agg_metrics['weighted_avg']},
        'per_class_mean': {name: {m: np.mean(agg_metrics['per_class'][name][m]) for m in agg_metrics['per_class'][name]} for name in target_names},
        'per_class_std': {name: {m: np.std(agg_metrics['per_class'][name][m]) for m in agg_metrics['per_class'][name]} for name in target_names}
    }
    return final_metrics

cat_final_metrics = aggregate_metrics(cat_metrics_by_fold, target_names)
xgb_final_metrics = aggregate_metrics(xgb_metrics_by_fold, target_names)


print("\n--- Training Final Models on Complete Training Set ---")
scaler_final = StandardScaler()
X_train_scaled_final = pd.DataFrame(scaler_final.fit_transform(X_train_full), columns=X_train_full.columns)
X_test_scaled_final = pd.DataFrame(scaler_final.transform(X_test), columns=X_test.columns)

joblib.dump(scaler_final, scaler_save_path)
print("Final feature scaler saved successfully.")

# CatBoost
start_time_cat = time.time()
cat_model_final = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=8, loss_function='MultiClass',
                                     eval_metric='MultiClass', random_seed=42, early_stopping_rounds=50, verbose=0)
cat_model_final.fit(
    Pool(X_train_scaled_final, y_train_full, feature_names=list(X_train_scaled_final.columns)),
    eval_set=Pool(X_test_scaled_final, y_test),
    verbose=False
)
end_time_cat = time.time()
cat_training_time = end_time_cat - start_time_cat
y_pred_cat_final = cat_model_final.predict(X_test_scaled_final).flatten()
y_pred_probs_cat_final = cat_model_final.predict_proba(X_test_scaled_final)
joblib.dump(cat_model_final, catboost_model_save_path)

# XGBoost
start_time_xgb = time.time()
xgb_model_final = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=8, objective='multi:softmax',
                                num_class=len(target_names), eval_metric='merror', random_state=42, n_jobs=-1,
                                early_stopping_rounds=50, use_label_encoder=False)
xgb_model_final.fit(X_train_scaled_final, y_train_full, eval_set=[(X_test_scaled_final, y_test)], verbose=False)
end_time_xgb = time.time()
xgb_training_time = end_time_xgb - start_time_xgb
y_pred_xgb_final = xgb_model_final.predict(X_test_scaled_final)
y_pred_probs_xgb_final = xgb_model_final.predict_proba(X_test_scaled_final)
joblib.dump(xgb_model_final, xgboost_model_save_path)
print("Final models and scaler saved successfully.")

results = {
    'CatBoost_CV_Aggregated': cat_final_metrics,
    'XGBoost_CV_Aggregated': xgb_final_metrics,
    'CatBoost_Final': {
        'accuracy': accuracy_score(y_test, y_pred_cat_final),
        'report': classification_report(y_test, y_pred_cat_final, target_names=target_names, output_dict=True, zero_division=0),
        'confusion_matrix': confusion_matrix(y_test, y_pred_cat_final).tolist(),
        'training_time': cat_training_time,
    },
    'XGBoost_Final': {
        'accuracy': accuracy_score(y_test, y_pred_xgb_final),
        'report': classification_report(y_test, y_pred_xgb_final, target_names=target_names, output_dict=True, zero_division=0),
        'confusion_matrix': confusion_matrix(y_test, y_pred_xgb_final).tolist(),
        'training_time': xgb_training_time,
    }
}


print("\n" + "="*80)
print("             Comparative K-Fold Cross-Validation Metrics")
print("="*80)

def print_cv_metrics_table(metrics, model_name):
    print(f"\nModel: {model_name} (Mean and Std Dev over {N_FOLDS} folds)")
    print("-" * 75)
    print(f"{'Metric':<25}{'Mean Score':<20}{'Std Dev':<15}")
    print("-" * 75)
    print(f"{'Macro Avg Precision':<25}{metrics['macro_avg_mean']['precision']:.4f}{metrics['macro_avg_std']['precision']:.4f}")
    print(f"{'Macro Avg Recall':<25}{metrics['macro_avg_mean']['recall']:.4f}{metrics['macro_avg_std']['recall']:.4f}")
    print(f"{'Macro Avg F1-Score':<25}{metrics['macro_avg_mean']['f1-score']:.4f}{metrics['macro_avg_std']['f1-score']:.4f}")
    print(f"{'Accuracy':<25}{metrics['accuracy_mean']:.4f}{metrics['accuracy_std']:.4f}")
    print("-" * 75)

print_cv_metrics_table(results['CatBoost_CV_Aggregated'], 'CatBoost')
print_cv_metrics_table(results['XGBoost_CV_Aggregated'], 'XGBoost')

print("\n" + "="*80)
print("             Final Model Performance on Holdout Test Set")
print("="*80)
print(f"\nModel: CatBoost Final Model")
print(classification_report(y_test, y_pred_cat_final, target_names=target_names, zero_division=0))
print(f"\nModel: XGBoost Final Model")
print(classification_report(y_test, y_pred_xgb_final, target_names=target_names, zero_division=0))

# 1. Overall Performance Bar Chart (using final model metrics)
performance_data = {
    'CatBoost': [
        results['CatBoost_Final']['report']['accuracy'],
        results['CatBoost_Final']['report']['macro avg']['precision'],
        results['CatBoost_Final']['report']['macro avg']['recall'],
        results['CatBoost_Final']['report']['macro avg']['f1-score'],
    ],
    'XGBoost': [
        results['XGBoost_Final']['report']['accuracy'],
        results['XGBoost_Final']['report']['macro avg']['precision'],
        results['XGBoost_Final']['report']['macro avg']['recall'],
        results['XGBoost_Final']['report']['macro avg']['f1-score'],
    ]
}
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
x = np.arange(len(metrics))
width = 0.35
fig, ax = plt.subplots(figsize=(10, 6))
rects1 = ax.bar(x - width/2, performance_data['CatBoost'], width, label='CatBoost', color='#3256a8')
rects2 = ax.bar(x + width/2, performance_data['XGBoost'], width, label='XGBoost', color='#ff6e26')
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.4f}', xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=8)
autolabel(rects1)
autolabel(rects2)
ax.set_ylabel('Score')
ax.set_title('Final Comparative Macro-Average Performance Metrics')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.set_ylim(0.95, 1.01)
ax.legend()
fig.tight_layout()
plt.savefig('final_performance_bar_chart.png', dpi=PLT_DPI)
plt.close(fig)

# 2. Training Runtime Bar Chart
fig, ax = plt.subplots(figsize=(8, 6))
runtime_data = [results['CatBoost_Final']['training_time'], results['XGBoost_Final']['training_time']]
models = ['CatBoost', 'XGBoost']
ax.bar(models, runtime_data, color=['#3256a8', '#ff6e26'])
ax.set_ylabel('Training Time (s)')
ax.set_title('Comparative Training Runtime (Final Model)')
ax.grid(axis='y')
for i, v in enumerate(runtime_data):
    ax.text(i, v + 0.1, f'{v:.2f}s', ha='center', fontsize=10)
fig.tight_layout()
plt.savefig('final_runtime_bar_chart.png', dpi=PLT_DPI)
plt.close(fig)

# 3. Class-by-Class Performance Bar Chart
def create_per_class_bar_chart(metrics, filename):
    fig, ax = plt.subplots(figsize=(15, 8))
    n_classes = len(target_names)
    bar_width = 0.2
    index = np.arange(n_classes)

    cat_metrics = [results['CatBoost_Final']['report'][c][metrics] for c in target_names]
    xgb_metrics = [results['XGBoost_Final']['report'][c][metrics] for c in target_names]

    rects1 = ax.bar(index - bar_width, cat_metrics, bar_width, label='CatBoost', color='#3256a8')
    rects2 = ax.bar(index + bar_width, xgb_metrics, bar_width, label='XGBoost', color='#ff6e26')

    ax.set_ylabel(metrics.capitalize())
    ax.set_title(f'Final Comparative Per-Class {metrics.capitalize()} Scores')
    ax.set_xticks(index)
    ax.set_xticklabels(target_names, rotation=45, ha='right')
    ax.set_ylim(0.95, 1.01)
    ax.legend()
    fig.tight_layout()
    plt.savefig(filename, dpi=PLT_DPI)
    plt.close(fig)

create_per_class_bar_chart('precision', 'final_per_class_precision_bar_chart.png')
create_per_class_bar_chart('recall', 'final_per_class_recall_bar_chart.png')
create_per_class_bar_chart('f1-score', 'final_per_class_f1_score_bar_chart.png')


# 4. Zoomed-In ROC and PR Curves
colors = plt.cm.get_cmap('Dark2', len(target_names))
line_styles = ['-', '--']
fig, ax = plt.subplots(figsize=(12, 10))
for i, fault_type in enumerate(target_names):

    fpr_cat, tpr_cat, _ = roc_curve(y_test == i, y_pred_probs_cat_final[:, i])
    fpr_xgb, tpr_xgb, _ = roc_curve(y_test == i, y_pred_probs_xgb_final[:, i])

    roc_auc_cat = auc(fpr_cat, tpr_cat)
    roc_auc_xgb = auc(fpr_xgb, tpr_xgb)

    ax.plot(fpr_cat, tpr_cat, lw=2, color=colors(i), linestyle=line_styles[0],
            label=f'CatBoost ROC of {fault_type} (area = {roc_auc_cat:.4f})')
    ax.plot(fpr_xgb, tpr_xgb, lw=2, color=colors(i), linestyle=line_styles[1],
            label=f'XGBoost ROC of {fault_type} (area = {roc_auc_xgb:.4f})')

ax.set_xlim([0.0, 0.05])
ax.set_ylim([0.95, 1.00])
ax.set_xlabel('False Positive Rate (FPR)')
ax.set_ylabel('True Positive Rate (TPR)')
ax.set_title('Final Zoomed-In ROC Curves')
ax.legend(loc='lower right', fontsize='small')
fig.tight_layout()
plt.savefig('final_zoomed_roc_curves.png', dpi=PLT_DPI)
plt.close(fig)

fig, ax = plt.subplots(figsize=(12, 10))
for i, fault_type in enumerate(target_names):

    precision_cat, recall_cat, _ = precision_recall_curve(y_test == i, y_pred_probs_cat_final[:, i])
    precision_xgb, recall_xgb, _ = precision_recall_curve(y_test == i, y_pred_probs_xgb_final[:, i])

    avg_precision_cat = average_precision_score(y_test == i, y_pred_probs_cat_final[:, i])
    avg_precision_xgb = average_precision_score(y_test == i, y_pred_probs_xgb_final[:, i])

    ax.plot(recall_cat, precision_cat, lw=2, color=colors(i), linestyle=line_styles[0],
            label=f'CatBoost PR of {fault_type} (area = {avg_precision_cat:.4f})')
    ax.plot(recall_xgb, precision_xgb, lw=2, color=colors(i), linestyle=line_styles[1],
            label=f'XGBoost PR of {fault_type} (area = {avg_precision_xgb:.4f})')

ax.set_xlim([0.95, 1.00])
ax.set_ylim([0.95, 1.00])
ax.set_xlabel('Recall')
ax.set_ylabel('Precision')
ax.set_title('Final Zoomed-In Precision-Recall Curves')
ax.legend(loc='lower left', fontsize='small')
fig.tight_layout()
plt.savefig('final_zoomed_pr_curves.png', dpi=PLT_DPI)
plt.close(fig)

# 5. Comparative SHAP Feature Importance
explainer_cat = shap.TreeExplainer(cat_model_final)
shap_values_cat = explainer_cat.shap_values(X_test_scaled_final)
shap.summary_plot(shap_values_cat, X_test_scaled_final, plot_type="bar", class_names=target_names, show=False)
plt.title('Final SHAP Feature Importance (CatBoost)', fontsize=16)
fig.tight_layout()
plt.savefig('final_catboost_shap_summary_plot.png', dpi=PLT_DPI)
plt.close('all')

explainer_xgb = shap.TreeExplainer(xgb_model_final)
shap_values_xgb = explainer_xgb.shap_values(X_test_scaled_final)
shap.summary_plot(shap_values_xgb, X_test_scaled_final, plot_type="bar", class_names=target_names, show=False)
plt.title('Final SHAP Feature Importance (XGBoost)', fontsize=16)
fig.tight_layout()
plt.savefig('final_xgboost_shap_summary_plot.png', dpi=PLT_DPI)
plt.close('all')

# 6. Confusion Matrices
def plot_confusion_matrix(y_true, y_pred, labels, model_name, ax):
    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation='vertical')
    ax.set_title(f'Final Confusion Matrix - {model_name}')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
plot_confusion_matrix(y_test, y_pred_cat_final, target_names, 'CatBoost', ax1)
plot_confusion_matrix(y_test, y_pred_xgb_final, target_names, 'XGBoost', ax2)
fig.tight_layout()
plt.savefig('final_confusion_matrices_comparison.png', dpi=PLT_DPI)
plt.close(fig)


# --- Save Results for Reporting ---
print("\n--- Saving Comparative Results and Reports ---")
with open(results_save_path, 'w') as f:
    json.dump(results, f, indent=4)
print(f"Final comparative results saved to {results_save_path}")
print("\nAnalysis complete.")